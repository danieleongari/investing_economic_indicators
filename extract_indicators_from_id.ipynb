{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract economic table and scrape informations for each indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time\n",
    "sleep_time_sec = 1\n",
    "CHROME_OPTIONS = Options()\n",
    "CHROME_OPTIONS.add_argument('--no-sandbox')\n",
    "CHROME_OPTIONS.add_argument('--headless')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve information from each page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Index: 200\n",
      " - URL: https://www.investing.com/economic-calendar/machine-tool-orders-200 \n",
      " - Name: machine tool orders\n",
      " - Country: Japan\n",
      " - Title: Japan Machine Tool Orders YoY\n",
      " - Source: Japan Machine Tool Builders' Association\n",
      " - Source_link: http://www.jmtba.or.jp/english/ \n",
      " - Currency: JPY\n",
      " - Importance: 1\n",
      " - Description: Machine Tool Orders measures the change in the total value of new orders placed with machine tool manufacturers. Two versions of this report are released, preliminary and final. The preliminary report had the biggest impact.A higher than expected reading should be taken as positive/bullish for the JPY, while a lower than expected reading should be taken as negative/bearish for the JPY.\n"
     ]
    }
   ],
   "source": [
    "# Use Selenium on the URL page to extract page-info\n",
    "def scrape_page(url):\n",
    "    blacklist = [\n",
    "        \"We're temporarily down for maintenance; Please check back soon...\" # Some pages take several seconds to load and then throw this error\n",
    "    ]\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    browser = webdriver.Chrome(service=service, options=CHROME_OPTIONS)\n",
    "    \n",
    "    try:\n",
    "        browser.get(url)\n",
    "    except:\n",
    "        print(f\"Skipped: {url} to browser.get(url) failure\")\n",
    "        res = {k: None for k in ['Title', 'Source', 'Source_link', 'Currency', 'Description']}\n",
    "        soup = None\n",
    "        return res, soup\n",
    "    \n",
    "    time.sleep(sleep_time_sec) # To leave the time for the page to load \n",
    "    html = browser.page_source\n",
    "    browser.close()\n",
    "    soup = bs(html,'html.parser')\n",
    "\n",
    "    res = {}\n",
    "\n",
    "    res[\"Index\"] = int(url.split(\"/\")[-1])\n",
    "    \n",
    "    # Link to the source, e.g., adp-nonfarm-employment-change-1\n",
    "    link_tag = soup.find('link', rel='canonical')\n",
    "    \n",
    "    if not link_tag: # page not existing e.g., https://www.investing.com/economic-calendar/002\n",
    "        return res, None\n",
    "    \n",
    "    res[\"URL\"] = link_tag['href'] + \" \" # e.g., \"https://www.investing.com/economic-calendar/adp-nonfarm-employment-change-1 \" NOTE: leave a space at the end to make it clickable in the csv\n",
    "    res[\"Name\"] = res[\"URL\"].split(\"/\")[-1].rpartition('-')[0].replace(\"-\", \" \") # e.g., adp nonfarm employment change\n",
    "    \n",
    "    country_label = soup.find('span', string='Country:')\n",
    "    country_tag = country_label.find_next('i', class_='ceFlags')\n",
    "    res[\"Country\"] = country_tag['title']\n",
    "\n",
    "    res['Title'] = soup.find('title').text.strip()\n",
    "    if res['Title'] in blacklist:\n",
    "        print(f\"Skipped: {url} due to `{res['Title']}`\")\n",
    "        res = {k: None for k in ['Title', 'Source', 'Source_link', 'Currency', 'Description']}\n",
    "        return res, soup\n",
    "\n",
    "    source_span = soup.find('span', string='Source:')\n",
    "    res['Source'] = source_span.find_next_sibling('span').find('a')['title']\n",
    "    res['Source_link'] = source_span.find_next_sibling('span').find('a')['href'] + \" \" # NOTE: leave a space at the end to make it clickable in the csv\n",
    "\n",
    "    curr_span = soup.find('span', string='Currency:')\n",
    "    res['Currency'] = curr_span.find_next_sibling('span').text\n",
    "    \n",
    "    stars = soup.find_all('i', class_='grayFullBullishIcon')\n",
    "    res[\"Importance\"] = len(stars)\n",
    "\n",
    "    overViewBox_div = soup.find('div', class_='overViewBox')\n",
    "    overViewBox_left = overViewBox_div.find('div', class_='left')\n",
    "    if overViewBox_left:\n",
    "        res['Description'] = (\n",
    "            overViewBox_left\n",
    "            .get_text()\n",
    "            .replace(\"<br>\", \" \")\n",
    "            .replace(\"\\n\", \" \")\n",
    "        )\n",
    "    else:\n",
    "        res['Description'] = None\n",
    "\n",
    "    return res, soup\n",
    "\n",
    "# Test\n",
    "url = \"https://www.investing.com/economic-calendar/200\"\n",
    "res, soup = scrape_page(url)\n",
    "for k, v in res.items():\n",
    "    print(f\" - {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.investing.com/economic-calendar/001 already scraped as https://www.investing.com/economic-calendar/adp-nonfarm-employment-change-1 \n",
      "https://www.investing.com/economic-calendar/002 failed to scrape\n",
      "https://www.investing.com/economic-calendar/003 failed to scrape\n",
      "https://www.investing.com/economic-calendar/004 already scraped as https://www.investing.com/economic-calendar/all-industries-activity-index.-4 \n",
      "https://www.investing.com/economic-calendar/005 already scraped as https://www.investing.com/economic-calendar/anz-commodity-price-index-5 \n",
      "https://www.investing.com/economic-calendar/006 already scraped as https://www.investing.com/economic-calendar/average-cash-earnings-6 \n",
      "https://www.investing.com/economic-calendar/007 already scraped as https://www.investing.com/economic-calendar/average-earnings-index-bonus-7 \n",
      "https://www.investing.com/economic-calendar/008 already scraped as https://www.investing.com/economic-calendar/average-hourly-earnings-8 \n",
      "https://www.investing.com/economic-calendar/009 already scraped as https://www.investing.com/economic-calendar/gross-mortgage-approvals-9 \n",
      "https://www.investing.com/economic-calendar/010 already scraped as https://www.investing.com/economic-calendar/beige-book-10 \n",
      "https://www.investing.com/economic-calendar/011 already scraped as https://www.investing.com/economic-calendar/boc-review-11 \n",
      "https://www.investing.com/economic-calendar/012 already scraped as https://www.investing.com/economic-calendar/boc-deputy-governor-duguay-speaks-12 \n",
      "https://www.investing.com/economic-calendar/013 already scraped as https://www.investing.com/economic-calendar/boc-deputy-governor-jenkins-speaks-13 \n",
      "https://www.investing.com/economic-calendar/014 already scraped as https://www.investing.com/economic-calendar/boe-deputy-governor-lomax-speaks-14 \n",
      "https://www.investing.com/economic-calendar/015 already scraped as https://www.investing.com/economic-calendar/boe-inflation-report-15 \n",
      "https://www.investing.com/economic-calendar/016 already scraped as https://www.investing.com/economic-calendar/boe-official-jenkinson-speaks-16 \n",
      "https://www.investing.com/economic-calendar/017 already scraped as https://www.investing.com/economic-calendar/boj-governor-fukui-speaks-17 \n",
      "https://www.investing.com/economic-calendar/018 already scraped as https://www.investing.com/economic-calendar/brc-retail-sales-monitor-18 \n",
      "https://www.investing.com/economic-calendar/019 already scraped as https://www.investing.com/economic-calendar/brc-shop-price-index-19 \n",
      "https://www.investing.com/economic-calendar/020 failed to scrape\n",
      "https://www.investing.com/economic-calendar/021 already scraped as https://www.investing.com/economic-calendar/bsi-large-manufacturing-conditions-21 \n",
      "https://www.investing.com/economic-calendar/022 failed to scrape\n",
      "https://www.investing.com/economic-calendar/023 already scraped as https://www.investing.com/economic-calendar/building-consents-23 \n",
      "https://www.investing.com/economic-calendar/024 already scraped as https://www.investing.com/economic-calendar/building-permits-24 \n",
      "https://www.investing.com/economic-calendar/025 already scraped as https://www.investing.com/economic-calendar/building-permits-25 \n",
      "https://www.investing.com/economic-calendar/026 failed to scrape\n",
      "https://www.investing.com/economic-calendar/027 already scraped as https://www.investing.com/economic-calendar/anz-business-confidence-27 \n",
      "https://www.investing.com/economic-calendar/028 failed to scrape\n",
      "https://www.investing.com/economic-calendar/029 already scraped as https://www.investing.com/economic-calendar/business-inventories-29 \n",
      "https://www.investing.com/economic-calendar/030 already scraped as https://www.investing.com/economic-calendar/business-investment-30 \n",
      "https://www.investing.com/economic-calendar/031 already scraped as https://www.investing.com/economic-calendar/capacity-utilization-rate-31 \n",
      "https://www.investing.com/economic-calendar/032 already scraped as https://www.investing.com/economic-calendar/capital-spending-32 \n",
      "https://www.investing.com/economic-calendar/033 already scraped as https://www.investing.com/economic-calendar/cbi-distributive-trades-realized-33 \n",
      "https://www.investing.com/economic-calendar/034 already scraped as https://www.investing.com/economic-calendar/cbi-industrial-trends-orders-34 \n",
      "https://www.investing.com/economic-calendar/035 already scraped as https://www.investing.com/economic-calendar/ppi-35 \n",
      "https://www.investing.com/economic-calendar/036 already scraped as https://www.investing.com/economic-calendar/chancellor-darling-speaks-36 \n",
      "https://www.investing.com/economic-calendar/037 already scraped as https://www.investing.com/economic-calendar/chicago-fed-president-evans-speaks-37 \n",
      "https://www.investing.com/economic-calendar/038 already scraped as https://www.investing.com/economic-calendar/chicago-pmi-38 \n",
      "https://www.investing.com/economic-calendar/039 already scraped as https://www.investing.com/economic-calendar/claimant-count-change-39 \n",
      "https://www.investing.com/economic-calendar/040 already scraped as https://www.investing.com/economic-calendar/company-gross-operating-profits-40 \n",
      "https://www.investing.com/economic-calendar/041 failed to scrape\n",
      "https://www.investing.com/economic-calendar/042 failed to scrape\n",
      "https://www.investing.com/economic-calendar/043 failed to scrape\n",
      "https://www.investing.com/economic-calendar/044 already scraped as https://www.investing.com/economic-calendar/construction-pmi-44 \n",
      "https://www.investing.com/economic-calendar/045 already scraped as https://www.investing.com/economic-calendar/construction-spending-45 \n",
      "https://www.investing.com/economic-calendar/046 already scraped as https://www.investing.com/economic-calendar/construction-work-done-46 \n",
      "https://www.investing.com/economic-calendar/047 failed to scrape\n",
      "https://www.investing.com/economic-calendar/048 already scraped as https://www.investing.com/economic-calendar/cb-consumer-confidence-48 \n",
      "https://www.investing.com/economic-calendar/049 failed to scrape\n",
      "https://www.investing.com/economic-calendar/050 failed to scrape\n",
      "https://www.investing.com/economic-calendar/051 failed to scrape\n",
      "https://www.investing.com/economic-calendar/052 already scraped as https://www.investing.com/economic-calendar/consumer-credit-52 \n",
      "https://www.investing.com/economic-calendar/053 failed to scrape\n",
      "https://www.investing.com/economic-calendar/054 already scraped as https://www.investing.com/economic-calendar/consumption-indicator-54 \n",
      "https://www.investing.com/economic-calendar/055 already scraped as https://www.investing.com/economic-calendar/core-cpi-55 \n",
      "https://www.investing.com/economic-calendar/056 already scraped as https://www.investing.com/economic-calendar/core-cpi-56 \n",
      "https://www.investing.com/economic-calendar/057 already scraped as https://www.investing.com/economic-calendar/core-cpi-57 \n",
      "https://www.investing.com/economic-calendar/058 failed to scrape\n",
      "https://www.investing.com/economic-calendar/059 already scraped as https://www.investing.com/economic-calendar/core-durable-goods-orders-59 \n",
      "https://www.investing.com/economic-calendar/060 already scraped as https://www.investing.com/economic-calendar/core-machinery-orders-60 \n",
      "https://www.investing.com/economic-calendar/061 already scraped as https://www.investing.com/economic-calendar/core-pce-price-index-61 \n",
      "https://www.investing.com/economic-calendar/062 already scraped as https://www.investing.com/economic-calendar/core-ppi-62 \n",
      "https://www.investing.com/economic-calendar/063 already scraped as https://www.investing.com/economic-calendar/core-retail-sales-63 \n",
      "https://www.investing.com/economic-calendar/064 already scraped as https://www.investing.com/economic-calendar/core-retail-sales-64 \n",
      "https://www.investing.com/economic-calendar/065 failed to scrape\n",
      "https://www.investing.com/economic-calendar/066 failed to scrape\n",
      "https://www.investing.com/economic-calendar/067 already scraped as https://www.investing.com/economic-calendar/cpi-67 \n",
      "https://www.investing.com/economic-calendar/068 already scraped as https://www.investing.com/economic-calendar/cpi-68 \n",
      "https://www.investing.com/economic-calendar/069 already scraped as https://www.investing.com/economic-calendar/cpi-69 \n",
      "https://www.investing.com/economic-calendar/070 already scraped as https://www.investing.com/economic-calendar/cpi-70 \n",
      "https://www.investing.com/economic-calendar/071 failed to scrape\n",
      "https://www.investing.com/economic-calendar/072 already scraped as https://www.investing.com/economic-calendar/cpi-72 \n",
      "https://www.investing.com/economic-calendar/073 already scraped as https://www.investing.com/economic-calendar/cpi-73 \n",
      "https://www.investing.com/economic-calendar/074 already scraped as https://www.investing.com/economic-calendar/credit-card-spending-74 \n",
      "https://www.investing.com/economic-calendar/075 already scraped as https://www.investing.com/economic-calendar/eia-crude-oil-inventories-75 \n",
      "https://www.investing.com/economic-calendar/076 already scraped as https://www.investing.com/economic-calendar/cspi-76 \n",
      "https://www.investing.com/economic-calendar/077 already scraped as https://www.investing.com/economic-calendar/adjusted-current-account-77 \n",
      "https://www.investing.com/economic-calendar/078 already scraped as https://www.investing.com/economic-calendar/current-account-78 \n",
      "https://www.investing.com/economic-calendar/079 already scraped as https://www.investing.com/economic-calendar/current-account-79 \n",
      "https://www.investing.com/economic-calendar/080 already scraped as https://www.investing.com/economic-calendar/current-account-80 \n",
      "https://www.investing.com/economic-calendar/081 already scraped as https://www.investing.com/economic-calendar/current-account-81 \n",
      "https://www.investing.com/economic-calendar/082 already scraped as https://www.investing.com/economic-calendar/current-account-82 \n",
      "https://www.investing.com/economic-calendar/083 already scraped as https://www.investing.com/economic-calendar/current-account-83 \n",
      "https://www.investing.com/economic-calendar/084 already scraped as https://www.investing.com/economic-calendar/discount-rate-84 \n",
      "https://www.investing.com/economic-calendar/085 already scraped as https://www.investing.com/economic-calendar/total-vehicle-sales-85 \n",
      "https://www.investing.com/economic-calendar/086 already scraped as https://www.investing.com/economic-calendar/durable-goods-orders-86 \n",
      "https://www.investing.com/economic-calendar/087 failed to scrape\n",
      "https://www.investing.com/economic-calendar/088 already scraped as https://www.investing.com/economic-calendar/ecb-president-trichet-speaks-88 \n",
      "https://www.investing.com/economic-calendar/089 failed to scrape\n",
      "https://www.investing.com/economic-calendar/090 failed to scrape\n",
      "https://www.investing.com/economic-calendar/091 already scraped as https://www.investing.com/economic-calendar/economy-watchers-current-index-91 \n",
      "https://www.investing.com/economic-calendar/092 failed to scrape\n",
      "https://www.investing.com/economic-calendar/093 already scraped as https://www.investing.com/economic-calendar/employment-change-93 \n",
      "https://www.investing.com/economic-calendar/094 already scraped as https://www.investing.com/economic-calendar/employment-change-94 \n",
      "https://www.investing.com/economic-calendar/095 already scraped as https://www.investing.com/economic-calendar/employment-change-95 \n",
      "https://www.investing.com/economic-calendar/096 already scraped as https://www.investing.com/economic-calendar/employment-change-96 \n",
      "https://www.investing.com/economic-calendar/097 failed to scrape\n",
      "https://www.investing.com/economic-calendar/098 already scraped as https://www.investing.com/economic-calendar/employment-level-98 \n",
      "https://www.investing.com/economic-calendar/099 already scraped as https://www.investing.com/economic-calendar/existing-home-sales-99 \n",
      "https://www.investing.com/economic-calendar/100 already scraped as https://www.investing.com/economic-calendar/factory-orders-100 \n",
      "https://www.investing.com/economic-calendar/101 already scraped as https://www.investing.com/economic-calendar/fed-chairman-bernanke-speaks-101 \n",
      "https://www.investing.com/economic-calendar/102 already scraped as https://www.investing.com/economic-calendar/fed-governor-kohn-speaks-102 \n",
      "https://www.investing.com/economic-calendar/103 already scraped as https://www.investing.com/economic-calendar/fed-governor-kroszner-speaks-103 \n",
      "https://www.investing.com/economic-calendar/104 already scraped as https://www.investing.com/economic-calendar/fed-governor-mishkin-speaks-104 \n",
      "https://www.investing.com/economic-calendar/105 already scraped as https://www.investing.com/economic-calendar/fed-governor-warsh-speaks-105 \n",
      "https://www.investing.com/economic-calendar/106 failed to scrape\n",
      "https://www.investing.com/economic-calendar/107 failed to scrape\n",
      "https://www.investing.com/economic-calendar/108 already scraped as https://www.investing.com/economic-calendar/fomc-meeting-minutes-108 \n",
      "https://www.investing.com/economic-calendar/109 already scraped as https://www.investing.com/economic-calendar/foreign-securities-purchases-109 \n",
      "https://www.investing.com/economic-calendar/110 already scraped as https://www.investing.com/economic-calendar/fpi-110 \n",
      "https://www.investing.com/economic-calendar/111 already scraped as https://www.investing.com/economic-calendar/french-consumer-spending-111 \n",
      "https://www.investing.com/economic-calendar/112 already scraped as https://www.investing.com/economic-calendar/french-cpi-112 \n",
      "https://www.investing.com/economic-calendar/113 already scraped as https://www.investing.com/economic-calendar/french-gdp-113 \n",
      "https://www.investing.com/economic-calendar/114 already scraped as https://www.investing.com/economic-calendar/french-government-budget-balance-114 \n",
      "https://www.investing.com/economic-calendar/115 already scraped as https://www.investing.com/economic-calendar/french-industrial-production-115 \n",
      "https://www.investing.com/economic-calendar/116 failed to scrape\n",
      "https://www.investing.com/economic-calendar/117 already scraped as https://www.investing.com/economic-calendar/french-trade-balance-117 \n",
      "https://www.investing.com/economic-calendar/118 already scraped as https://www.investing.com/economic-calendar/g20-meetings-118 \n",
      "https://www.investing.com/economic-calendar/119 already scraped as https://www.investing.com/economic-calendar/gdp-119 \n",
      "https://www.investing.com/economic-calendar/120 already scraped as https://www.investing.com/economic-calendar/gdp-120 \n",
      "https://www.investing.com/economic-calendar/121 already scraped as https://www.investing.com/economic-calendar/gdp-121 \n",
      "https://www.investing.com/economic-calendar/122 already scraped as https://www.investing.com/economic-calendar/swiss-gdp-122 \n",
      "https://www.investing.com/economic-calendar/123 already scraped as https://www.investing.com/economic-calendar/gdp-123 \n",
      "https://www.investing.com/economic-calendar/124 already scraped as https://www.investing.com/economic-calendar/gdp-124 \n",
      "https://www.investing.com/economic-calendar/125 already scraped as https://www.investing.com/economic-calendar/gdp-125 \n",
      "https://www.investing.com/economic-calendar/126 already scraped as https://www.investing.com/economic-calendar/gdp-price-index-126 \n",
      "https://www.investing.com/economic-calendar/127 failed to scrape\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already scraped as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m res, soup \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records([res])]) \u001b[38;5;66;03m# Equivalent to df.append(res, ignore_index=True)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(filename, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mscrape_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_page\u001b[39m(url):\n\u001b[1;32m      3\u001b[0m     blacklist \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre temporarily down for maintenance; Please check back soon...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Some pages take several seconds to load and then throw this error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     ]\n\u001b[0;32m----> 6\u001b[0m     service \u001b[38;5;241m=\u001b[39m Service(\u001b[43mChromeDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m     browser \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mservice, options\u001b[38;5;241m=\u001b[39mCHROME_OPTIONS)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/webdriver_manager/chrome.py:40\u001b[0m, in \u001b[0;36mChromeDriverManager.install\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m     driver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_driver_binary_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     os\u001b[38;5;241m.\u001b[39mchmod(driver_path, \u001b[38;5;241m0o755\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver_path\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/webdriver_manager/core/manager.py:40\u001b[0m, in \u001b[0;36mDriverManager._get_driver_binary_path\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n\u001b[1;32m     39\u001b[0m os_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_os_type()\n\u001b[0;32m---> 40\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_driver_download_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m binary_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_manager\u001b[38;5;241m.\u001b[39msave_file_to_cache(driver, file)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/webdriver_manager/core/download_manager.py:32\u001b[0m, in \u001b[0;36mWDMDownloadManager.download_file\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     30\u001b[0m log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDriver downloading response is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_filename_from_url(url)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/webdriver_manager/core/file_manager.py:12\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, stream, file_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream, file_name):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__stream \u001b[38;5;241m=\u001b[39m stream\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_name \u001b[38;5;241m=\u001b[39m file_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = \"economic_indicators.csv\"\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=['Index', 'URL', 'Name', 'Country', 'Title', 'Source', 'Source_link', 'Importance', 'Currency', 'Description'])\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "for i in range(1, 1000):\n",
    "    url = f\"https://www.investing.com/economic-calendar/{i:03d}\"\n",
    "    \n",
    "    row = df[df['Index'] == i]\n",
    "    if row.shape[0] > 0:\n",
    "        if pd.isna(row['URL'].values[0]):\n",
    "            print(f\"{url} failed to scrape\")\n",
    "        else:\n",
    "            print(f\"{url} already scraped as {row['URL'].values[0]}\")\n",
    "        continue\n",
    "    \n",
    "    res, soup = scrape_page(url)\n",
    "    df = pd.concat([df, pd.DataFrame.from_records([res])]) # Equivalent to df.append(res, ignore_index=True)\n",
    "    df.to_csv(filename, index=False)\n",
    "    if \"URL\" in res:\n",
    "        print(f\"{res['URL']} scraped\")\n",
    "    else:\n",
    "        print(f\"{url} failed to scrape\")\n",
    "    time.sleep(sleep_time_sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the README\n",
    "readme_text = \"\"\"# Investing.com Economic Indicators\n",
    "\n",
    "List of all the economic indicators from [Investing.com](https://www.investing.com/).\n",
    "\n",
    "Available as a [CSV table](economic_indicators.csv).\n",
    "\"\"\"\n",
    "\n",
    "readme_text += \"\\n## Most Important Indicators (three stars)\\n\\n\"\n",
    "\n",
    "filename = \"economic_indicators.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "df = df.sort_values(by=['Country', 'Index'], ascending=True)\n",
    "\n",
    "for i, row in df[df['Importance'] == 3].iterrows():\n",
    "    readme_text += f\"1. {row['Country']} - [{row['Title']}]({row['URL'].strip()})\\n\"\n",
    "    \n",
    "readme_text += \"\\n## Middle Importance Indicators (two stars)\\n\\n\"\n",
    "\n",
    "for i, row in df[df['Importance'] == 2].iterrows():\n",
    "    readme_text += f\"1. {row['Country']} - [{row['Title']}]({row['URL'].strip()})\\n\"\n",
    "    \n",
    "readme_text += \"\\n## Least Important Indicators (one star)\\n\\n\"\n",
    "\n",
    "for i, row in df[df['Importance'] == 1].iterrows():\n",
    "    readme_text += f\"1. {row['Country']} - [{row['Title']}]({row['URL'].strip()})\\n\"\n",
    "\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(readme_text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2540cd9198af0b9eaea5a5cb83e008828d2d08e9e651b172f5d346fcf048848a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
